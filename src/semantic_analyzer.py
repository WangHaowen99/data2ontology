"""LLM-based semantic analysis for ontology generation.

This module provides configurable LLM prompts for analyzing database tables
and columns to infer their business meaning.
"""

import os
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field

# é»˜è®¤ Prompt æ¨¡æ¿
DEFAULT_TABLE_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æžä¸“å®¶ã€‚è¯·åˆ†æžä»¥ä¸‹æ•°æ®åº“è¡¨ä¿¡æ¯ï¼ŒæŽ¨æ–­å®ƒåœ¨ä¸šåŠ¡ç³»ç»Ÿä¸­ä»£è¡¨çš„å®žä½“å«ä¹‰ã€‚

## è¡¨ä¿¡æ¯
- è¡¨å: {table_name}
- è¡¨æ³¨é‡Š: {table_comment}
- åˆ—æ•°: {column_count}
- é¢„ä¼°è¡Œæ•°: {row_count}

## åˆ—ä¿¡æ¯
{columns_info}

## æ•°æ®æ ·æœ¬
{sample_data}

## è¯·åˆ†æžå¹¶ç»™å‡ºï¼š
1. **ä¸šåŠ¡å®žä½“åç§°**ï¼šè¿™ä¸ªè¡¨ä»£è¡¨ä»€ä¹ˆä¸šåŠ¡å®žä½“ï¼ˆç”¨ä¸­æ–‡å‘½åï¼‰
2. **å®žä½“æè¿°**ï¼šç”¨1-2å¥è¯æè¿°è¿™ä¸ªå®žä½“åœ¨ä¸šåŠ¡ä¸­çš„ä½œç”¨
3. **æ ¸å¿ƒå±žæ€§åˆ†æž**ï¼šåˆ†æžæ¯ä¸ªåˆ—çš„ä¸šåŠ¡å«ä¹‰ï¼ˆä¸­æ–‡ï¼‰

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›žï¼š
```json
{{
    "entity_name_cn": "ä¸­æ–‡å®žä½“åç§°",
    "entity_description": "å®žä½“çš„ä¸šåŠ¡æè¿°",
    "properties": [
        {{
            "column_name": "åˆ—å",
            "business_name": "ä¸šåŠ¡åç§°ï¼ˆä¸­æ–‡ï¼‰",
            "business_description": "ä¸šåŠ¡å«ä¹‰æè¿°"
        }}
    ]
}}
```"""

DEFAULT_RELATIONSHIP_ANALYSIS_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ•°æ®åˆ†æžä¸“å®¶ã€‚è¯·åˆ†æžä»¥ä¸‹ä¸¤ä¸ªè¡¨ä¹‹é—´çš„å…³ç³»ï¼ŒæŽ¨æ–­å®ƒä»¬åœ¨ä¸šåŠ¡ä¸­çš„å…³è”å«ä¹‰ã€‚

## æºè¡¨: {source_table}
æè¿°: {source_description}

## ç›®æ ‡è¡¨: {target_table}
æè¿°: {target_description}

## å…³è”å­—æ®µ
æºè¡¨åˆ—: {source_column}
ç›®æ ‡è¡¨åˆ—: {target_column}

## è¯·åˆ†æžå¹¶ç»™å‡ºï¼š
1. **å…³ç³»åç§°**ï¼šè¿™ä¸ªå…³ç³»çš„ä¸šåŠ¡åç§°ï¼ˆä¸­æ–‡åŠ¨è¯çŸ­è¯­ï¼Œå¦‚"å±žäºŽ"ã€"åŒ…å«"ã€"å…³è”"ï¼‰
2. **å…³ç³»æè¿°**ï¼šæè¿°è¿™ä¸¤ä¸ªå®žä½“ä¹‹é—´çš„ä¸šåŠ¡å…³ç³»

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¿”å›žï¼š
```json
{{
    "relationship_name_cn": "å…³ç³»åç§°ï¼ˆä¸­æ–‡ï¼‰",
    "relationship_description": "å…³ç³»çš„ä¸šåŠ¡æè¿°"
}}
```"""


@dataclass
class LLMConfig:
    """LLM configuration."""
    provider: str = "openai"  # openai, azure, local
    api_key: str = ""
    api_base: str = ""
    model: str = "gpt-4o-mini"
    temperature: float = 0.3
    max_tokens: int = 2000
    
    # Prompt æ¨¡æ¿
    table_analysis_prompt: str = DEFAULT_TABLE_ANALYSIS_PROMPT
    relationship_analysis_prompt: str = DEFAULT_RELATIONSHIP_ANALYSIS_PROMPT


class SemanticAnalyzer:
    """LLM-based semantic analyzer for database tables."""
    
    def __init__(self, config: LLMConfig = None):
        self.config = config or LLMConfig()
        self._client = None
        
    def _get_client(self):
        """Get or create LLM client."""
        if self._client:
            return self._client
            
        if self.config.provider == "openai":
            try:
                from openai import OpenAI
                api_key = self.config.api_key or os.getenv("OPENAI_API_KEY")
                if not api_key:
                    return None
                self._client = OpenAI(
                    api_key=api_key,
                    base_url=self.config.api_base or os.getenv("OPENAI_API_BASE")
                )
            except ImportError:
                return None
        return self._client
    
    def analyze_table(
        self,
        table_name: str,
        columns: List[Dict[str, Any]],
        sample_data: List[Dict[str, Any]] = None,
        table_comment: str = None,
        row_count: int = None
    ) -> Optional[Dict[str, Any]]:
        """Analyze a table to infer its business meaning.
        
        Args:
            table_name: Name of the table
            columns: List of column info dicts
            sample_data: Sample rows from the table
            table_comment: Table comment if any
            row_count: Estimated row count
            
        Returns:
            Analysis result dict or None if LLM not available
        """
        client = self._get_client()
        if not client:
            # è¿”å›žåŸºäºŽè§„åˆ™çš„åˆ†æž
            return self._rule_based_table_analysis(table_name, columns, sample_data)
        
        # æž„å»ºåˆ—ä¿¡æ¯
        columns_info = "\n".join([
            f"- {col['name']} ({col['data_type']})"
            + (f" - PK" if col.get('is_primary_key') else "")
            + (f" - {col.get('comment')}" if col.get('comment') else "")
            for col in columns
        ])
        
        # æž„å»ºæ ·æœ¬æ•°æ®
        sample_str = "æ— æ ·æœ¬æ•°æ®"
        if sample_data and len(sample_data) > 0:
            sample_rows = sample_data[:3]  # æœ€å¤š3è¡Œ
            sample_str = json.dumps(sample_rows, ensure_ascii=False, indent=2, default=str)
        
        # å¡«å…… prompt
        prompt = self.config.table_analysis_prompt.format(
            table_name=table_name,
            table_comment=table_comment or "æ— ",
            column_count=len(columns),
            row_count=row_count or "æœªçŸ¥",
            columns_info=columns_info,
            sample_data=sample_str
        )
        
        try:
            response = client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æžå¸ˆï¼Œæ“…é•¿åˆ†æžæ•°æ®åº“ç»“æž„å¹¶æŽ¨æ–­ä¸šåŠ¡å«ä¹‰ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens
            )
            
            content = response.choices[0].message.content
            
            # æå– JSON
            json_start = content.find('{')
            json_end = content.rfind('}') + 1
            if json_start >= 0 and json_end > json_start:
                return json.loads(content[json_start:json_end])
                
        except Exception as e:
            print(f"LLM analysis failed: {e}")
        
        return self._rule_based_table_analysis(table_name, columns, sample_data)
    
    def _rule_based_table_analysis(
        self,
        table_name: str,
        columns: List[Dict[str, Any]],
        sample_data: List[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Fallback rule-based analysis when LLM is not available."""
        # åŸºäºŽè¡¨åæŽ¨æ–­å®žä½“åç§°
        entity_name = self._infer_entity_name(table_name)
        
        # åˆ†æžæ¯ä¸ªåˆ—
        properties = []
        for col in columns:
            prop = {
                "column_name": col["name"],
                "business_name": self._infer_column_name(col["name"]),
                "business_description": self._infer_column_description(
                    col["name"], 
                    col["data_type"],
                    col.get("comment"),
                    col.get("is_primary_key", False)
                )
            }
            properties.append(prop)
        
        return {
            "entity_name_cn": entity_name,
            "entity_description": f"{entity_name}ä¿¡æ¯è®°å½•",
            "properties": properties
        }
    
    def _infer_entity_name(self, table_name: str) -> str:
        """Infer Chinese entity name from table name."""
        name_lower = table_name.lower()
        
        # ç§»é™¤å¸¸è§å‰ç¼€
        prefixes = ['raw_', 't_', 'tbl_', 'tb_', 'dim_', 'fact_', 'ods_', 'dwd_', 'dws_', 'ads_']
        for prefix in prefixes:
            if name_lower.startswith(prefix):
                name_lower = name_lower[len(prefix):]
                break
        
        # å¸¸è§å®žä½“åæ˜ å°„
        entity_mappings = {
            'user': 'ç”¨æˆ·', 'users': 'ç”¨æˆ·', 'account': 'è´¦æˆ·', 'accounts': 'è´¦æˆ·',
            'order': 'è®¢å•', 'orders': 'è®¢å•', 'product': 'äº§å“', 'products': 'äº§å“',
            'customer': 'å®¢æˆ·', 'customers': 'å®¢æˆ·', 'item': 'é¡¹ç›®', 'items': 'é¡¹ç›®',
            'category': 'ç±»åˆ«', 'categories': 'ç±»åˆ«', 'department': 'éƒ¨é—¨',
            'employee': 'å‘˜å·¥', 'employees': 'å‘˜å·¥', 'staff': 'å‘˜å·¥',
            'project': 'é¡¹ç›®', 'projects': 'é¡¹ç›®', 'task': 'ä»»åŠ¡', 'tasks': 'ä»»åŠ¡',
            'log': 'æ—¥å¿—', 'logs': 'æ—¥å¿—', 'record': 'è®°å½•', 'records': 'è®°å½•',
            'config': 'é…ç½®', 'setting': 'è®¾ç½®', 'settings': 'è®¾ç½®',
            'file': 'æ–‡ä»¶', 'files': 'æ–‡ä»¶', 'document': 'æ–‡æ¡£', 'documents': 'æ–‡æ¡£',
            'message': 'æ¶ˆæ¯', 'messages': 'æ¶ˆæ¯', 'notification': 'é€šçŸ¥',
            'payment': 'æ”¯ä»˜', 'payments': 'æ”¯ä»˜', 'transaction': 'äº¤æ˜“',
            'inventory': 'åº“å­˜', 'stock': 'åº“å­˜', 'warehouse': 'ä»“åº“',
            'supplier': 'ä¾›åº”å•†', 'vendor': 'ä¾›åº”å•†', 'partner': 'åˆä½œä¼™ä¼´',
            'contract': 'åˆåŒ', 'agreement': 'åè®®',
            'equipment': 'è®¾å¤‡', 'device': 'è®¾å¤‡', 'machine': 'æœºå™¨',
            'defect': 'ç¼ºé™·', 'defects': 'ç¼ºé™·', 'issue': 'é—®é¢˜', 'bug': 'ç¼ºé™·',
            'work_order': 'å·¥å•', 'workorder': 'å·¥å•', 'ticket': 'å·¥å•',
            'listing': 'åˆ—è¡¨é¡¹', 'listings': 'åˆ—è¡¨é¡¹',
            'district': 'åŒºåŸŸ', 'area': 'åŒºåŸŸ', 'region': 'åœ°åŒº',
        }
        
        for eng, chn in entity_mappings.items():
            if eng in name_lower:
                return chn
        
        # é»˜è®¤ä½¿ç”¨è¡¨å
        return table_name.replace('_', ' ').title()
    
    def _infer_column_name(self, column_name: str) -> str:
        """Infer Chinese column name."""
        name_lower = column_name.lower()
        
        column_mappings = {
            'id': 'æ ‡è¯†', 'uuid': 'å”¯ä¸€æ ‡è¯†', 'code': 'ç¼–ç ',
            'name': 'åç§°', 'title': 'æ ‡é¢˜', 'label': 'æ ‡ç­¾',
            'description': 'æè¿°', 'desc': 'æè¿°', 'content': 'å†…å®¹', 'text': 'æ–‡æœ¬',
            'status': 'çŠ¶æ€', 'state': 'çŠ¶æ€', 'type': 'ç±»åž‹', 'category': 'ç±»åˆ«',
            'created_at': 'åˆ›å»ºæ—¶é—´', 'updated_at': 'æ›´æ–°æ—¶é—´', 'deleted_at': 'åˆ é™¤æ—¶é—´',
            'create_time': 'åˆ›å»ºæ—¶é—´', 'update_time': 'æ›´æ–°æ—¶é—´',
            'start_time': 'å¼€å§‹æ—¶é—´', 'end_time': 'ç»“æŸæ—¶é—´',
            'price': 'ä»·æ ¼', 'amount': 'é‡‘é¢', 'cost': 'æˆæœ¬', 'total': 'æ€»è®¡',
            'quantity': 'æ•°é‡', 'qty': 'æ•°é‡', 'count': 'æ•°é‡',
            'user_id': 'ç”¨æˆ·ID', 'order_id': 'è®¢å•ID', 'product_id': 'äº§å“ID',
            'parent_id': 'çˆ¶çº§ID', 'level': 'å±‚çº§', 'sort': 'æŽ’åº',
            'is_active': 'æ˜¯å¦æ¿€æ´»', 'is_deleted': 'æ˜¯å¦åˆ é™¤', 'enabled': 'æ˜¯å¦å¯ç”¨',
            'email': 'é‚®ç®±', 'phone': 'ç”µè¯', 'mobile': 'æ‰‹æœº', 'address': 'åœ°å€',
            'remark': 'å¤‡æ³¨', 'note': 'å¤‡æ³¨', 'comment': 'å¤‡æ³¨',
            'version': 'ç‰ˆæœ¬', 'priority': 'ä¼˜å…ˆçº§',
        }
        
        for eng, chn in column_mappings.items():
            if eng == name_lower or name_lower.endswith(f'_{eng}'):
                return chn
        
        return column_name.replace('_', ' ').title()
    
    def _infer_column_description(
        self,
        column_name: str,
        data_type: str,
        comment: str = None,
        is_primary_key: bool = False
    ) -> str:
        """Infer column business description."""
        if comment:
            return comment
        
        name_lower = column_name.lower()
        
        # åŸºäºŽåˆ—åæ¨¡å¼æŽ¨æ–­
        if is_primary_key or name_lower == 'id':
            return "è®°å½•çš„å”¯ä¸€æ ‡è¯†ç¬¦"
        if name_lower.endswith('_id') or name_lower.endswith('id'):
            ref_name = name_lower.replace('_id', '').replace('id', '')
            return f"å…³è”{self._infer_entity_name(ref_name)}çš„æ ‡è¯†"
        if 'time' in name_lower or 'date' in name_lower or 'at' in name_lower:
            return "æ—¶é—´æˆ³è®°å½•"
        if 'status' in name_lower or 'state' in name_lower:
            return "å½“å‰çŠ¶æ€æ ‡è¯†"
        if 'name' in name_lower or 'title' in name_lower:
            return "æ˜¾ç¤ºåç§°"
        if 'desc' in name_lower or 'content' in name_lower:
            return "è¯¦ç»†æè¿°ä¿¡æ¯"
        if 'price' in name_lower or 'amount' in name_lower or 'cost' in name_lower:
            return "é‡‘é¢æ•°å€¼"
        if 'count' in name_lower or 'qty' in name_lower or 'quantity' in name_lower:
            return "æ•°é‡ç»Ÿè®¡"
        if name_lower.startswith('is_') or name_lower.startswith('has_'):
            return "å¸ƒå°”æ ‡è®°"
        
        # åŸºäºŽæ•°æ®ç±»åž‹
        type_lower = data_type.lower()
        if 'bool' in type_lower:
            return "æ˜¯/å¦æ ‡è®°"
        if 'int' in type_lower or 'numeric' in type_lower:
            return "æ•°å€¼"
        if 'text' in type_lower or 'varchar' in type_lower or 'char' in type_lower:
            return "æ–‡æœ¬ä¿¡æ¯"
        if 'time' in type_lower or 'date' in type_lower:
            return "æ—¶é—´è®°å½•"
        if 'json' in type_lower:
            return "ç»“æž„åŒ–æ•°æ®"
        
        return f"{self._infer_column_name(column_name)}å­—æ®µ"


def generate_semantic_report(
    ontology,
    table_analyses: Dict[str, Dict[str, Any]],
    relationship_analyses: Dict[str, Dict[str, Any]] = None
) -> str:
    """Generate a semantic report from analysis results.
    
    Args:
        ontology: The generated ontology
        table_analyses: Dict mapping table names to their analysis results
        relationship_analyses: Optional dict of relationship analyses
        
    Returns:
        Markdown formatted report
    """
    report = "# æœ¬ä½“è¯­ä¹‰åˆ†æžæŠ¥å‘Š\n\n"
    
    # æ‘˜è¦
    report += "## ðŸ“Š æ¦‚è§ˆ\n\n"
    report += f"| æŒ‡æ ‡ | æ•°é‡ |\n"
    report += f"|------|------|\n"
    report += f"| ä¸šåŠ¡å®žä½“ | {ontology.object_type_count} |\n"
    report += f"| å®žä½“å…³ç³» | {ontology.link_type_count} |\n"
    report += f"| æ€»å±žæ€§æ•° | {sum(len(obj.properties) for obj in ontology.object_types)} |\n\n"
    
    # å®žä½“åˆ†æž
    report += "## ðŸ¢ ä¸šåŠ¡å®žä½“\n\n"
    
    for obj in ontology.object_types:
        table_name = obj.source_table.split('.')[-1] if '.' in obj.source_table else obj.source_table
        analysis = table_analyses.get(table_name, {})
        
        entity_name = analysis.get("entity_name_cn", obj.name)
        entity_desc = analysis.get("entity_description", obj.description)
        
        report += f"### {entity_name} ({obj.name})\n\n"
        report += f"**ä¸šåŠ¡æè¿°**: {entity_desc}\n\n"
        report += f"**æ•°æ®æ¥æº**: `{obj.source_table}`\n\n"
        
        # å±žæ€§è¡¨æ ¼
        report += "| å±žæ€§ | ä¸šåŠ¡åç§° | ç±»åž‹ | è¯´æ˜Ž |\n"
        report += "|------|---------|------|------|\n"
        
        prop_analyses = {p["column_name"]: p for p in analysis.get("properties", [])}
        
        for prop in obj.properties:
            col_name = prop.source_column or prop.name
            prop_info = prop_analyses.get(col_name, {})
            business_name = prop_info.get("business_name", prop.name)
            business_desc = prop_info.get("business_description", prop.description)
            pk_mark = " ðŸ”‘" if prop.is_primary_key else ""
            
            report += f"| {prop.name}{pk_mark} | {business_name} | {prop.data_type} | {business_desc} |\n"
        
        report += "\n"
    
    # å…³ç³»åˆ†æž
    if ontology.link_types:
        report += "## ðŸ”— å®žä½“å…³ç³»\n\n"
        
        for link in ontology.link_types:
            rel_key = f"{link.source_object_type}-{link.target_object_type}"
            rel_analysis = (relationship_analyses or {}).get(rel_key, {})
            
            rel_name = rel_analysis.get("relationship_name_cn", link.name)
            rel_desc = rel_analysis.get("relationship_description", link.description)
            
            report += f"- **{link.source_object_type}** â†’ *{rel_name}* â†’ **{link.target_object_type}**\n"
            report += f"  - {rel_desc}\n"
        
        report += "\n"
    
    return report


# é…ç½®æ–‡ä»¶è·¯å¾„
PROMPTS_CONFIG_PATH = None


def get_prompts_config_path() -> str:
    """Get the path to prompts config file."""
    global PROMPTS_CONFIG_PATH
    if PROMPTS_CONFIG_PATH:
        return PROMPTS_CONFIG_PATH
    
    from pathlib import Path
    home = Path.home()
    config_dir = home / ".data2ontology"
    config_dir.mkdir(exist_ok=True)
    return str(config_dir / "prompts_config.json")


def load_prompts_config() -> Dict[str, str]:
    """Load prompts configuration from file."""
    config_path = get_prompts_config_path()
    if os.path.exists(config_path):
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            pass
    
    return {
        "table_analysis_prompt": DEFAULT_TABLE_ANALYSIS_PROMPT,
        "relationship_analysis_prompt": DEFAULT_RELATIONSHIP_ANALYSIS_PROMPT
    }


def save_prompts_config(config: Dict[str, str]):
    """Save prompts configuration to file."""
    config_path = get_prompts_config_path()
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
