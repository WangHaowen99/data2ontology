"""
Auto Pipeline Builder - ä¸»ç¨‹åºå…¥å£

ä¸€ä¸ªç±»ä¼¼ Palantir Pipeline Builder çš„è‡ªåŠ¨åŒ–æ•°æ®ç®¡é“æž„å»ºå·¥å…·ã€‚
"""

import sys
import json
from pathlib import Path
from typing import Optional

import click
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich.progress import Progress, SpinnerColumn, TextColumn
from dotenv import load_dotenv

from src.config import AppConfig, DatabaseConfig, AnalysisConfig, OutputConfig, Neo4jConfig
from src.metadata_extractor import MetadataExtractor
from src.relationship_analyzer import RelationshipAnalyzer
from src.pipeline_builder import PipelineBuilder
from src.ontology_generator import OntologyGenerator
from src.report_generator import ReportGenerator
from src.neo4j_exporter import export_ontology_to_neo4j

console = Console()


@click.command()
# PostgreSQL - Set defaults to None to allow .env override
@click.option("--host", "-h", default=None, help="PostgreSQL ä¸»æœºåœ°å€ (é»˜è®¤: localhost)")
@click.option("--port", "-p", default=None, type=int, help="PostgreSQL ç«¯å£ (é»˜è®¤: 5432)")
@click.option("--database", "-d", required=False, help="æ•°æ®åº“åç§° (å¯é€‰ï¼Œå¯ç»ç”±çŽ¯å¢ƒå˜é‡é…ç½®)")
@click.option("--user", "-u", required=False, help="æ•°æ®åº“ç”¨æˆ·å (å¯é€‰ï¼Œå¯ç»ç”±çŽ¯å¢ƒå˜é‡é…ç½®)")
@click.option("--password", "-P", required=False, help="æ•°æ®åº“å¯†ç  (å¯é€‰ï¼Œå¯ç»ç”±çŽ¯å¢ƒå˜é‡é…ç½®)", hide_input=True)
@click.option("--schema", "-s", default=None, help="è¦åˆ†æžçš„ Schema (é»˜è®¤: public)")
# Output
@click.option("--output", "-o", default="./output", help="è¾“å‡ºç›®å½•")
# Environment
@click.option("--env-file", "-e", default=".env", help=".env æ–‡ä»¶è·¯å¾„")
# Neo4j
@click.option("--neo4j-uri", default=None, help="Neo4j URI (é»˜è®¤: bolt://localhost:7687)")
@click.option("--neo4j-user", default=None, help="Neo4j ç”¨æˆ·å (é»˜è®¤: neo4j)")
@click.option("--neo4j-password", default=None, help="Neo4j å¯†ç  (é»˜è®¤: ç©º)")
@click.option("--export-neo4j", is_flag=True, help="æ˜¯å¦å¯¼å‡ºåˆ° Neo4j")
# Misc
@click.option("--verbose", "-v", is_flag=True, help="è¯¦ç»†è¾“å‡º")
def main(
    host: Optional[str], 
    port: Optional[int], 
    database: Optional[str], 
    user: Optional[str], 
    password: Optional[str], 
    schema: Optional[str], 
    output: str, 
    env_file: str, 
    neo4j_uri: Optional[str], 
    neo4j_user: Optional[str], 
    neo4j_password: Optional[str], 
    export_neo4j: bool,
    verbose: bool
):
    """
    Auto Pipeline Builder - è‡ªåŠ¨æ•°æ®ç®¡é“æž„å»ºå·¥å…·
    
    ä»Ž PostgreSQL æ•°æ®åº“è¯»å–å…ƒæ•°æ®ï¼Œè‡ªåŠ¨åˆ†æžè¡¨å…³ç³»ï¼Œç”Ÿæˆæ•°æ®ç®¡é“å’Œ Ontology åŽŸåž‹ã€‚
    """
    # Create configuration from args (prioritizing args > env > defaults)
    # We pass the .env file path to from_env inside config if needed, but pydantic-settings handles it.
    # Actually, config.py loads .env if we call from_env via BaseSettings logic or load_dotenv manually.
    # main.py calls load_dotenv first.
    
    if Path(env_file).exists():
        load_dotenv(env_file)
    
    # We construct AppConfig using from_args which carefully merges provided args
    config = AppConfig.from_args(
        host=host,
        port=port,
        database=database,
        user=user,
        password=password,
        schema=schema,
        output_dir=output,
        neo4j_uri=neo4j_uri,
        neo4j_user=neo4j_user,
        neo4j_password=neo4j_password,
        # other flags
    )
    
    # Validation: Ensure critical DB config is present (either from args or env)
    if not config.database.database or not config.database.user or not config.database.password:
        console.print("[bold red]é”™è¯¯: å¿…é¡»æä¾›æ•°æ®åº“åç§°ã€ç”¨æˆ·åå’Œå¯†ç  (é€šè¿‡å‚æ•°æˆ– .env)[/bold red]")
        sys.exit(1)

    console.print(Panel.fit(
        "[bold blue]Auto Pipeline Builder[/bold blue]\n"
        "è‡ªåŠ¨æ•°æ®ç®¡é“æž„å»ºå·¥å…·",
        border_style="blue"
    ))
    
    console.print(f"\n[cyan]è¿žæŽ¥æ•°æ®åº“:[/cyan] {config.database.host}:{config.database.port}/{config.database.database}")
    console.print(f"[cyan]åˆ†æž Schema:[/cyan] {config.database.schema}")
    console.print(f"[cyan]è¾“å‡ºç›®å½•:[/cyan] {config.output.output_dir}")
    if export_neo4j:
         console.print(f"[cyan]Neo4j å¯¼å‡º:[/cyan] {config.neo4j.uri}")
    console.print("")

    try:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            console=console,
        ) as progress:
            
            # Step 1: Extract metadata
            task = progress.add_task("[cyan]æå–æ•°æ®åº“å…ƒæ•°æ®...", total=None)
            extractor = MetadataExtractor(config.database, config.analysis)
            metadata = extractor.extract_metadata()
            extractor.close()
            progress.update(task, description=f"[green]âœ“ å‘çŽ° {metadata.table_count} ä¸ªè¡¨ï¼Œ{metadata.column_count} åˆ—")
            
            # Step 2: Analyze relationships
            task = progress.add_task("[cyan]åˆ†æžè¡¨é—´å…³ç³»...", total=None)
            analyzer = RelationshipAnalyzer(config.analysis)
            metadata = analyzer.analyze(metadata)
            rel_count = len(metadata.detected_relationships)
            progress.update(task, description=f"[green]âœ“ æ£€æµ‹åˆ° {rel_count} ä¸ªå…³ç³»")
            
            # Step 3: Build pipelines
            task = progress.add_task("[cyan]ç”Ÿæˆæ•°æ®ç®¡é“...", total=None)
            builder = PipelineBuilder(metadata, config.analysis)
            datasets = builder.generate_datasets()
            pipelines = []
            for ds in datasets:
                try:
                    tables = list(set(c.source_table for c in ds.columns))
                    if len(tables) >= 2:
                        pipeline = builder.create_pipeline(
                            name=ds.name.replace("_dataset", "_pipeline"),
                            source_tables=tables,
                        )
                        pipelines.append(pipeline)
                except Exception:
                    pass
            progress.update(task, description=f"[green]âœ“ ç”Ÿæˆ {len(pipelines)} ä¸ªç®¡é“ï¼Œ{len(datasets)} ä¸ªæ•°æ®é›†")
            
            # Step 4: Generate ontology
            task = progress.add_task("[cyan]ç”Ÿæˆ Ontology åŽŸåž‹...", total=None)
            ont_generator = OntologyGenerator(metadata, config.analysis)
            ontology = ont_generator.generate()
            progress.update(task, description=f"[green]âœ“ åˆ›å»º {ontology.object_type_count} ä¸ªå®žä½“ç±»åž‹ï¼Œ{ontology.link_type_count} ä¸ªå…³ç³»ç±»åž‹")
            
            # Step 5: Export to Neo4j (Optional)
            if export_neo4j:
                task = progress.add_task("[cyan]å¯¼å‡ºè‡³ Neo4j (å«æ•°æ®åŒæ­¥)...", total=None)
                stats = export_ontology_to_neo4j(ontology, config.neo4j, config.database)
                
                desc = f"[green]âœ“ Neo4j å¯¼å‡ºå®Œæˆ: åˆ›å»º {stats.get('constraints_created', 0)} çº¦æŸ"
                if "nodes_created" in stats:
                    desc += f", {stats['nodes_created']} èŠ‚ç‚¹"
                if "relationships_created" in stats:
                    desc += f", {stats['relationships_created']} å…³ç³»"
                progress.update(task, description=desc)

            # Step 6: Generate reports
            task = progress.add_task("[cyan]ç”Ÿæˆåˆ†æžæŠ¥å‘Š...", total=None)
            report_generator = ReportGenerator(config.output)
            saved_paths = report_generator.save_all_reports(metadata, ontology, pipelines, datasets)
            progress.update(task, description=f"[green]âœ“ ç”Ÿæˆ {len(saved_paths)} ä¸ªæŠ¥å‘Šæ–‡ä»¶")
        
        # Print summary
        console.print("\n")
        console.print(Panel.fit(
            "[bold green]âœ“ å¤„ç†å®Œæˆï¼[/bold green]",
            border_style="green"
        ))
        
        # Summary table
        summary_table = Table(title="å¤„ç†æ‘˜è¦", show_header=True)
        summary_table.add_column("æŒ‡æ ‡", style="cyan")
        summary_table.add_column("æ•°å€¼", style="green")
        
        summary_table.add_row("è¡¨æ€»æ•°", str(metadata.table_count))
        summary_table.add_row("åˆ—æ€»æ•°", str(metadata.column_count))
        summary_table.add_row("å¤–é”®çº¦æŸ", str(metadata.foreign_key_count))
        summary_table.add_row("æ£€æµ‹åˆ°çš„å…³ç³»", str(len(metadata.detected_relationships)))
        summary_table.add_row("ç”Ÿæˆçš„å®žä½“ç±»åž‹", str(ontology.object_type_count))
        summary_table.add_row("ç”Ÿæˆçš„å…³ç³»ç±»åž‹", str(ontology.link_type_count))
        summary_table.add_row("ç”Ÿæˆçš„ç®¡é“", str(len(pipelines)))
        summary_table.add_row("ç”Ÿæˆçš„æ•°æ®é›†", str(len(datasets)))
        
        console.print(summary_table)
        
        # Output files
        console.print("\n[bold cyan]ç”Ÿæˆçš„æ–‡ä»¶:[/bold cyan]")
        for name, path in saved_paths.items():
            console.print(f"  ðŸ“„ {path}")
        
        # Recommendations
        recommendations = builder.get_join_recommendations()
        if recommendations and verbose:
            console.print("\n[bold cyan]å»ºè®®:[/bold cyan]")
            for rec in recommendations[:5]:
                if rec["type"] == "hub_table":
                    console.print(f"  â­ {rec['description']}")
                elif rec["type"] == "isolated_table":
                    console.print(f"  âš ï¸  {rec['description']}")
        
    except Exception as e:
        console.print(f"\n[bold red]é”™è¯¯:[/bold red] {str(e)}")
        if verbose:
            import traceback
            console.print(traceback.format_exc())
        sys.exit(1)


def run_from_config(config: AppConfig):
    """Run the pipeline builder from an AppConfig object.
    
    Args:
        config: Application configuration
    """
    # Extract metadata
    extractor = MetadataExtractor(config.database, config.analysis)
    metadata = extractor.extract_metadata()
    extractor.close()
    
    # Analyze relationships
    analyzer = RelationshipAnalyzer(config.analysis)
    metadata = analyzer.analyze(metadata)
    
    # Build pipelines
    builder = PipelineBuilder(metadata, config.analysis)
    datasets = builder.generate_datasets()
    pipelines = []
    for ds in datasets:
        try:
            tables = list(set(c.source_table for c in ds.columns))
            if len(tables) >= 2:
                pipeline = builder.create_pipeline(
                    name=ds.name.replace("_dataset", "_pipeline"),
                    source_tables=tables,
                )
                pipelines.append(pipeline)
        except Exception:
            pass
    
    # Generate ontology
    ont_generator = OntologyGenerator(metadata, config.analysis)
    ontology = ont_generator.generate()
    
    # Generate reports
    report_generator = ReportGenerator(config.output)
    saved_paths = report_generator.save_all_reports(metadata, ontology, pipelines, datasets)
    
    return {
        "metadata": metadata,
        "ontology": ontology,
        "pipelines": pipelines,
        "datasets": datasets,
        "reports": saved_paths,
    }


if __name__ == "__main__":
    main()
